#!/bin/bash
#COBALT -t 720 
#COBALT -n 1
#COBALT -q full-node
#COBALT -A SuperBERT
#COBALT -O cobalt_logs/job
#COBALT --attrs enable_ssh=1

# Change to 2 for Phase 2 training
PHASE=1

if [[ "$PHASE" -eq 1 ]]; then
	CONFIG=config/bert_pretraining_phase1_config.json
	DATA=/lus/theta-fs0/projects/SuperBERT/datasets/encoded/bert_masked_wikicorpus_en/phase1
else
	CONFIG=config/bert_pretraining_phase2_config.json
	DATA=/lus/theta-fs0/projects/SuperBERT/datasets/encoded/bert_masked_wikicorpus_en/phase2
fi

OUTPUT_DIR=results/bert_large_uncased_pretraining
# Add any argument overrides for the config file to $KWARGS
KWARGS=" --input_dir $DATA --output_dir $OUTPUT_DIR --config_file $CONFIG "

MASTER_RANK=$(head -n 1 $COBALT_NODEFILE)
RANKS=$(tr '\n' ' ' < $COBALT_NODEFILE)
NNODES=$(< $COBALT_NODEFILE wc -l)
NGPUS=8

# Launch the pytorch processes on each worker in NODEFILE
# Note we need to activate conda on the node as well
RANK=0
if [[ "$NNODES" -eq 1 ]]; then
    ./scripts/launch_pretraining.sh \
		--ngpus $NGPUS --nnodes $NNODES --kwargs $KWARGS 
else
    for NODE in $RANKS; do
		echo "Launching rank=$RANK, node=$NODE"
		if [[ "$RANK" -eq 0 ]]; then
			./scripts/launch_pretraining.sh \
				--ngpus $NGPUS --nnodes $NNODES --master $MASTER_RANK \
				--rank $RANK --kwargs $KWARGS &
		else
            ssh $NODE "cd $PWD; ./scripts/launch_pretraining.sh --ngpus $NGPUS --nnodes $NNODES --master $MASTER_RANK --rank $RANK --kwargs $KWARGS" &
		fi
		RANK=$((RANK+1))
    done
fi

wait

